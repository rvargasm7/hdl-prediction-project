---
title: "HDL Cholesterol Prediction - Interactive Analysis"
subtitle: "MAS 635 Machine Learning Methods - Midterm Project"
author:
  - name: "Rolando Vargas"
  - name: "Eleniz Espina"
  - name: "Bryce Leister"
affiliation: "University of Miami"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    code-fold: true
    code-summary: "Show code"
    code-tools: true
    theme: cosmo
    highlight-style: github
    df-print: paged
    embed-resources: true
    grid:
      sidebar-width: 300px
      body-width: 900px
      margin-width: 300px
execute:
  warning: false
  message: false
  cache: false
jupyter: python3
---

## Project Overview {#sec-overview}

This notebook presents a comprehensive machine learning analysis for predicting HDL (High-Density Lipoprotein) cholesterol levels using the NHANES dataset.

**GitHub Repository**: <https://github.com/rvargasm7/hdl_cholesterol_prediction>

### Objectives

1. **Exploratory Data Analysis**: Understand HDL cholesterol distribution and relationships
2. **Predictive Modeling**: Build and compare multiple ML/DL models
3. **Business Insights**: Extract actionable healthcare recommendations

### Dataset Information

- **Source**: [ASA South Florida Data Challenge](https://luminwin.github.io/ASASF/)
- **Training Samples**: ~7,000 observations
- **Test Samples**: ~3,000 observations
- **Features**: 100+ variables (demographics, body measures, dietary, lab values)
- **Target**: `LBDHDD_outcome` - Direct HDL Cholesterol (mg/dL)

---

## 1. Setup and Data Loading {#sec-setup}

### Required Libraries

```{python}
#| label: setup-libraries
#| code-summary: "Import libraries"

import warnings
import tempfile

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import pyreadr
from scipy import stats
from scipy.ndimage import uniform_filter1d

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from xgboost import XGBRegressor
from catboost import CatBoostRegressor

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Settings
warnings.filterwarnings('ignore')
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)
pd.set_option('display.max_columns', 100)
plt.style.use('seaborn-v0_8-whitegrid')

print("✓ All libraries loaded successfully!")
```

### Load Data

```{python}
#| label: load-data
#| code-summary: "Load training and test data"

def read_rds_from_url(url):
    """Download and read RDS file from URL."""
    response = requests.get(url)
    response.raise_for_status()
    with tempfile.NamedTemporaryFile(suffix=".rds") as temp_file:
        temp_file.write(response.content)
        temp_file.flush()
        obj = pyreadr.read_r(temp_file.name)
    return list(obj.values())[0]

# Load data
TRAIN_URL = "https://luminwin.github.io/ASASF/train.rds"
TEST_URL = "https://luminwin.github.io/ASASF/test.rds"

train_df = read_rds_from_url(TRAIN_URL)
test_df = read_rds_from_url(TEST_URL)

print(f"Training data shape: {train_df.shape}")
print(f"Test data shape: {test_df.shape}")
```

### Initial Data Exploration

```{python}
#| label: initial-exploration
#| code-summary: "First look at the data"

# Display first few rows
display(train_df.head())

# Basic statistics
TARGET_COL = 'LBDHDD_outcome'
print(f"\nTarget Variable ({TARGET_COL}) Statistics:")
print(f"  Mean: {train_df[TARGET_COL].mean():.2f} mg/dL")
print(f"  Std:  {train_df[TARGET_COL].std():.2f}")
print(f"  Min:  {train_df[TARGET_COL].min():.2f}")
print(f"  Max:  {train_df[TARGET_COL].max():.2f}")
print(f"  Missing: {train_df[TARGET_COL].isna().sum()}")
```

---

## 2. Exploratory Data Analysis {#sec-eda}

### Missing Data Analysis

```{python}
#| label: fig-missingness
#| fig-cap: "Missing data analysis showing features with highest missingness"
#| fig-width: 12
#| fig-height: 6

missing_pct = (train_df.isnull().sum() / len(train_df) * 100).sort_values(ascending=False)
missing_pct = missing_pct[missing_pct > 0]

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Top 30 missing features
top_missing = missing_pct.head(30)
axes[0].barh(range(len(top_missing)), top_missing.values, color='coral')
axes[0].set_yticks(range(len(top_missing)))
axes[0].set_yticklabels(top_missing.index, fontsize=8)
axes[0].set_xlabel('Missing Percentage (%)')
axes[0].set_title('Top 30 Features with Missing Values')
axes[0].invert_yaxis()

# Distribution of missingness
all_missing = train_df.isnull().sum() / len(train_df) * 100
axes[1].hist(all_missing, bins=30, color='steelblue', edgecolor='black')
axes[1].set_xlabel('Missing Percentage (%)')
axes[1].set_ylabel('Number of Features')
axes[1].set_title('Distribution of Missingness Across All Features')
axes[1].axvline(x=all_missing.mean(), color='red', linestyle='--',
                label=f'Mean: {all_missing.mean():.1f}%')
axes[1].legend()

plt.tight_layout()
plt.show()

print(f"Total features with missing values: {len(missing_pct)}")
```

::: {.callout-note}
## Key Observation
Many features have significant missingness (>50%), requiring robust imputation strategies.
:::

### Target Distribution

```{python}
#| label: fig-target-distribution
#| fig-cap: "HDL cholesterol distribution analysis"
#| fig-width: 14
#| fig-height: 5

target_data = train_df[TARGET_COL].dropna()

fig, axes = plt.subplots(1, 3, figsize=(14, 5))

# Histogram with KDE
axes[0].hist(target_data, bins=40, density=True, alpha=0.7,
             color='steelblue', edgecolor='black')
target_data.plot.kde(ax=axes[0], color='red', linewidth=2)
axes[0].set_xlabel('HDL Cholesterol (mg/dL)')
axes[0].set_ylabel('Density')
axes[0].set_title('HDL Distribution (Histogram + KDE)')
axes[0].axvline(target_data.mean(), color='green', linestyle='--',
                label=f'Mean: {target_data.mean():.1f}')
axes[0].axvline(target_data.median(), color='orange', linestyle='--',
                label=f'Median: {target_data.median():.1f}')
axes[0].legend()

# Box plot
axes[1].boxplot(target_data, vert=True)
axes[1].set_ylabel('HDL Cholesterol (mg/dL)')
axes[1].set_title('HDL Box Plot')

# Q-Q plot
stats.probplot(target_data, dist="norm", plot=axes[2])
axes[2].set_title('Q-Q Plot (Normality Check)')

plt.tight_layout()
plt.show()

print(f"Skewness: {target_data.skew():.3f}")
print(f"Kurtosis: {target_data.kurtosis():.3f}")
```

### Feature Correlations

```{python}
#| label: fig-correlations
#| fig-cap: "Correlation heatmap for top features"
#| fig-width: 12
#| fig-height: 10

numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
correlations = (train_df[numeric_cols].corr()[TARGET_COL]
                .drop(TARGET_COL).abs().sort_values(ascending=False))

top_features = correlations.head(20).index.tolist()
top_features.append(TARGET_COL)

corr_matrix = train_df[top_features].corr()

plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',
            center=0, linewidths=0.5, annot_kws={'size': 8})
plt.title('Correlation Heatmap: Top 20 Features', fontsize=14)
plt.tight_layout()
plt.show()

print("Top 10 correlations with HDL:")
print(correlations.head(10))
```

---

## 3. Relationship Analysis {#sec-relationships}

### HDL vs Body Mass Index

```{python}
#| label: fig-hdl-bmi
#| fig-cap: "Relationship between HDL and BMI"
#| fig-width: 10
#| fig-height: 6

bmi_col = [col for col in train_df.columns if 'BMXBMI' in col or 'BMI' in col.upper()]
if bmi_col:
    bmi_col = bmi_col[0]
    valid_data = train_df[[bmi_col, TARGET_COL]].dropna()

    plt.figure(figsize=(10, 6))
    plt.scatter(valid_data[bmi_col], valid_data[TARGET_COL],
                alpha=0.5, c='steelblue', s=30)

    # Trend line
    coeffs = np.polyfit(valid_data[bmi_col], valid_data[TARGET_COL], 1)
    poly = np.poly1d(coeffs)
    x_line = np.linspace(valid_data[bmi_col].min(), valid_data[bmi_col].max(), 100)
    plt.plot(x_line, poly(x_line), 'r-', linewidth=2,
             label=f'Trend: y = {coeffs[0]:.2f}x + {coeffs[1]:.2f}')

    corr = valid_data[bmi_col].corr(valid_data[TARGET_COL])
    plt.xlabel('BMI (kg/m²)')
    plt.ylabel('HDL Cholesterol (mg/dL)')
    plt.title(f'HDL vs BMI (Correlation: {corr:.3f})')
    plt.legend()
    plt.tight_layout()
    plt.show()
```

::: {.callout-important}
## Clinical Insight
Strong negative correlation between BMI and HDL levels - higher BMI associated with lower HDL cholesterol.
:::

### HDL by Demographic Groups

```{python}
#| label: fig-demographics
#| fig-cap: "HDL levels across demographic subgroups"
#| fig-width: 14
#| fig-height: 12

fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# By Sex
sex_col = [col for col in train_df.columns if 'RIAGENDR' in col or 'SEX' in col.upper()]
if sex_col:
    data = train_df[[sex_col[0], TARGET_COL]].dropna()
    sns.violinplot(data=data, x=sex_col[0], y=TARGET_COL, ax=axes[0,0], palette='Set2')
    axes[0,0].set_title('HDL by Sex')
    axes[0,0].set_xlabel('Sex (1=Male, 2=Female)')

# By Race/Ethnicity
race_col = [col for col in train_df.columns if 'RIDRETH' in col]
if race_col:
    data = train_df[[race_col[0], TARGET_COL]].dropna()
    sns.boxplot(data=data, x=race_col[0], y=TARGET_COL, ax=axes[0,1], palette='Set3')
    axes[0,1].set_title('HDL by Race/Ethnicity')

# By Smoking Status
smoke_col = [col for col in train_df.columns if 'SMQ020' in col or 'SMOKE' in col.upper()]
if smoke_col:
    data = train_df[[smoke_col[0], TARGET_COL]].dropna()
    sns.violinplot(data=data, x=smoke_col[0], y=TARGET_COL, ax=axes[1,0], palette='coolwarm')
    axes[1,0].set_title('HDL by Smoking Status')

# By Age Group
age_col = [col for col in train_df.columns if 'RIDAGEYR' in col or 'AGE' in col.upper()]
if age_col:
    data = train_df[[age_col[0], TARGET_COL]].dropna().copy()
    data['Age_Group'] = pd.cut(data[age_col[0]], bins=[0, 30, 45, 60, 100],
                                labels=['<30', '30-45', '45-60', '60+'])
    sns.boxplot(data=data, x='Age_Group', y=TARGET_COL, ax=axes[1,1], palette='viridis')
    axes[1,1].set_title('HDL by Age Group')

plt.tight_layout()
plt.show()
```

---

## 4. Predictive Modeling {#sec-modeling}

### Data Preprocessing

```{python}
#| label: preprocessing
#| code-summary: "Prepare data for modeling"

# Separate features and target
X = train_df.drop(columns=[TARGET_COL])
y = train_df[TARGET_COL]

# Identify feature types
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

print(f"Numeric features: {len(numeric_features)}")
print(f"Categorical features: {len(categorical_features)}")

# Create preprocessing pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Train-validation split
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

print(f"\nTraining set size: {len(X_train)}")
print(f"Validation set size: {len(X_val)}")
```

### Model Training & Evaluation

```{python}
#| label: model-training
#| code-summary: "Train baseline models"

def evaluate_model(y_true, y_pred, model_name):
    """Calculate model performance metrics."""
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f"\n{model_name} Results:")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE:  {mae:.4f}")
    print(f"  R²:   {r2:.4f}")

    return {'model': model_name, 'RMSE': rmse, 'MAE': mae, 'R2': r2}

results = []

# Linear Regression
lr_pipeline = Pipeline([('preprocessor', preprocessor),
                        ('regressor', LinearRegression())])
lr_pipeline.fit(X_train, y_train)
y_pred_lr = lr_pipeline.predict(X_val)
results.append(evaluate_model(y_val, y_pred_lr, 'Linear Regression'))

# Ridge Regression
ridge_pipeline = Pipeline([('preprocessor', preprocessor),
                          ('regressor', Ridge(alpha=1.0))])
ridge_pipeline.fit(X_train, y_train)
y_pred_ridge = ridge_pipeline.predict(X_val)
results.append(evaluate_model(y_val, y_pred_ridge, 'Ridge Regression'))

# Random Forest
rf_pipeline = Pipeline([('preprocessor', preprocessor),
                       ('regressor', RandomForestRegressor(
                           n_estimators=200, max_depth=15, random_state=RANDOM_STATE, n_jobs=-1))])
rf_pipeline.fit(X_train, y_train)
y_pred_rf = rf_pipeline.predict(X_val)
results.append(evaluate_model(y_val, y_pred_rf, 'Random Forest'))

# XGBoost
xgb_pipeline = Pipeline([('preprocessor', preprocessor),
                        ('regressor', XGBRegressor(
                            n_estimators=200, max_depth=6, random_state=RANDOM_STATE, n_jobs=-1))])
xgb_pipeline.fit(X_train, y_train)
y_pred_xgb = xgb_pipeline.predict(X_val)
results.append(evaluate_model(y_val, y_pred_xgb, 'XGBoost'))

# CatBoost
catboost_pipeline = Pipeline([('preprocessor', preprocessor),
                             ('regressor', CatBoostRegressor(
                                 iterations=200, depth=6, random_state=RANDOM_STATE, verbose=0))])
catboost_pipeline.fit(X_train, y_train)
y_pred_catboost = catboost_pipeline.predict(X_val)
results.append(evaluate_model(y_val, y_pred_catboost, 'CatBoost'))
```

### Model Comparison

```{python}
#| label: tbl-results
#| tbl-cap: "Model performance comparison"

results_df = pd.DataFrame(results).sort_values('RMSE')
display(results_df)
```

```{python}
#| label: fig-model-comparison
#| fig-cap: "Visual comparison of model performance"
#| fig-width: 14
#| fig-height: 5

fig, axes = plt.subplots(1, 3, figsize=(14, 5))

colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(results_df)))

axes[0].barh(results_df['model'], results_df['RMSE'], color=colors)
axes[0].set_xlabel('RMSE')
axes[0].set_title('Model Comparison - RMSE')
axes[0].invert_yaxis()

axes[1].barh(results_df['model'], results_df['MAE'], color=colors)
axes[1].set_xlabel('MAE')
axes[1].set_title('Model Comparison - MAE')
axes[1].invert_yaxis()

axes[2].barh(results_df['model'], results_df['R2'], color=colors[::-1])
axes[2].set_xlabel('R²')
axes[2].set_title('Model Comparison - R²')
axes[2].invert_yaxis()

plt.tight_layout()
plt.show()
```

---

## 5. Business Insights {#sec-insights}

### Key Findings

::: {.panel-tabset}

## Healthcare Applications

**Predictive Screening**:

- Identify individuals at risk without expensive blood tests
- Use demographic and lifestyle data for preliminary assessment
- Prioritize clinical resources for high-risk patients

**Intervention Planning**:

- Target modifiable risk factors (BMI, diet, exercise)
- Personalize health recommendations
- Monitor intervention effectiveness over time

## Risk Factors

**Modifiable Factors**:

1. **Body Mass Index**: Weight management programs
2. **Waist Circumference**: Focus on abdominal fat reduction
3. **Dietary Habits**: Nutrition counseling opportunities
4. **Physical Activity**: Exercise recommendations

**Non-Modifiable Factors**:

- Age, sex, ethnicity - for risk stratification

## Recommendations

**For Healthcare Providers**:

- Integrate models into EHR systems
- Use for population health management
- Validate predictions with actual tests
- Focus resources on predicted high-risk patients

**For Patients**:

- Focus on controllable factors
- Regular monitoring
- Lifestyle modifications
- Preventive care

:::

---

## 6. Conclusions {#sec-conclusions}

### Summary

This project successfully demonstrated that:

1. ✅ HDL cholesterol can be predicted using ML/DL with good accuracy
2. ✅ Ensemble methods (XGBoost, CatBoost) perform best
3. ✅ Body measurements show strongest predictive power
4. ✅ Model has practical healthcare applications

### Limitations

::: {.callout-warning}
- Target variable is noise-perturbed (privacy protection)
- Cannot replace clinical blood tests
- Requires validation on external datasets
- Model is screening tool, not diagnostic instrument
:::

### Future Work

- Feature engineering and interaction terms
- External validation studies
- Deployment as web application
- Integration with EHR systems
- Continuous model updating

---

## References {#sec-references}

1. CDC NHANES - [https://www.cdc.gov/nchs/nhanes/](https://www.cdc.gov/nchs/nhanes/)
2. ASA South Florida Challenge - [https://luminwin.github.io/ASASF/](https://luminwin.github.io/ASASF/)
3. XGBoost Documentation - [https://xgboost.readthedocs.io/](https://xgboost.readthedocs.io/)
4. CatBoost Documentation - [https://catboost.ai/docs/](https://catboost.ai/docs/)
5. TensorFlow/Keras - [https://www.tensorflow.org/](https://www.tensorflow.org/)

---

## Appendix: Code Repository {#sec-appendix}

**GitHub**: [https://github.com/rvargasm7/hdl_cholesterol_prediction](https://github.com/rvargasm7/hdl_cholesterol_prediction)

**Contents**:

- Full Jupyter notebook with all code
- Preprocessed data files
- Visualization outputs
- Model predictions
- Complete documentation

---

*Project completed as part of MAS 635 - Machine Learning Methods, University of Miami, February 2026*
